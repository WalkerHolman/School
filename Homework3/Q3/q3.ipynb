{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Q3 [35 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notices\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: <strong>REMOVE</strong> any print statements added to cells with \"#export\" that are used for debugging purposes befrore submitting because they will crash the autograder in Gradescope. Any additional cells can be used for testing purposes at the bottom. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remove any comment that says \"#export\" because that will crash the autograder in Gradescope. We use this comment to export your code in these cells for grading.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> import any additional libraries into this workbook.\n",
    "</div>\n",
    "\n",
    "All instructions, code comments, etc. in this notebook **are part of the assignment instructions**. That is, if there is instructions about completing a task in this notebook, that task is not optional.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    You <strong>must</strong> implement the following functions in this notebook to receive credit.\n",
    "</div>\n",
    "\n",
    "`user()` - 1 point\n",
    "\n",
    "`trip_statistics()` - 3 points\n",
    "\n",
    "`busiest_hour()` - 5 points\n",
    "\n",
    "`most_freq_pickup_locations()` - 5 points\n",
    "\n",
    "`avg_trip_distance_and_duration()` - 6 points\n",
    "\n",
    "`most_freq_peak_hour_fares()` - 10 points\n",
    "\n",
    "Each function will be auto-graded using different sets of parameters or data, to ensure that values are not hard-coded.  You may assume we will only use your code to work with data from the NYC-TLC dataset during auto-grading.\n",
    "\n",
    "In addition, you will also submit the resulting output csv from most_freq_peak_hour_fares() as output_large.csv.\n",
    "\n",
    "`output_large.csv` - 5 points\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remove or modify the following utility functions:\n",
    "</div>\n",
    "\n",
    "`load_data()`\n",
    "\n",
    "`main()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remodify the below cell. It contains the function for loading data and all imports, and the function for running your code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE ANYTHING IN THIS CELL ####\n",
    "\n",
    "def load_data(size='small'):\n",
    "    # Loads the data for this question. Do not change this function.\n",
    "    # This function should only be called with the parameter 'small' or 'large'\n",
    "    \n",
    "    if size != 'small' and size != 'large':\n",
    "        print(\"Invalid size parameter provided. Use only 'small' or 'large'.\")\n",
    "        return\n",
    "    \n",
    "    input_bucket = \"s3://cse6242-hw3-q3\"\n",
    "    \n",
    "    # Load Trip Data\n",
    "    trips_path = '/'+size+'/yellow_tripdata*'\n",
    "    trips = spark.read.csv(input_bucket + trips_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Load Zone Data\n",
    "    zones_path = '/'+size+'/taxi*'\n",
    "    zones = spark.read.csv(input_bucket + zones_path, header=True, inferSchema=True)\n",
    "    \n",
    "    return trips, zones\n",
    "    \n",
    "def main(size, bucket):\n",
    "    # Runs your functions\n",
    "    trips, zones = load_data(size=size)\n",
    "    \n",
    "    print(\"User:\", user())\n",
    "    print()\n",
    "    \n",
    "    print(\"Trip Statistics:\")\n",
    "    ts = trip_statistics(trips)\n",
    "    ts.show()\n",
    "    print()\n",
    "    \n",
    "    print(\"Busiest Hour:\")\n",
    "    bh = busiest_hour(trips)\n",
    "    bh.show(24)\n",
    "    print()\n",
    "    \n",
    "    print(\"Most Frequent Pickup Locations:\")\n",
    "    mfpl = most_freq_pickup_locations(trips)\n",
    "    mfpl.show()\n",
    "    print()\n",
    "    \n",
    "    print(\"Average Trip Distance and Duration:\")\n",
    "    atdd = avg_trip_distance_and_duration(trips)\n",
    "    atdd.show(n=24)\n",
    "    print()\n",
    "    \n",
    "    print(\"Most Frequent Peak Hour Fares:\")\n",
    "    mfphf = most_freq_peak_hour_fares(trips, zones)\n",
    "    mfphf.show()\n",
    "    mfphf.coalesce(1).write.option(\"header\",\"true\").mode(\"overwrite\").csv('{}/output_{}'.format(bucket, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the below functions for this assignment:\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> change any function inputs or outputs, and ensure that the dataframes your code returns align with the schema definitions commented in each function. Do <strong>NOT</strong> remove the #export comment from each of the code blocks either. This can prevent your code from being converted to a python file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 [1 pt] Update the `user()` function\n",
    "This function should return your GT username, eg: gburdell3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def user():\n",
    "    return 'jholman6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 [3 pts] Update the `trip_statistics()` function\n",
    "This function performs exploratory data analysis on the column trip_distance. Compute basic statistics (count, mean, stdev, min, max) for trip_distance. \n",
    "\n",
    "Example output formatting:\n",
    "\n",
    "```\n",
    "+-------+------------------+\n",
    "|summary|     trip_distance|\n",
    "+-------+------------------+\n",
    "|  count|           xxxxxxx|\n",
    "|   mean|           xxxxxxx|\n",
    "| stddev|           xxxxxxx|\n",
    "|    min|           xxxxxxx|\n",
    "|    max|           xxxxxxx|\n",
    "+-------+------------------+\n",
    "```\n",
    "Tip: Is there a PySpark Dataframe function you can use to solve this in a single line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trip_statistics(trips):\n",
    "    return trips.select(\"trip_distance\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 [5 pts] Update the `busiest_hour()` function\n",
    "\n",
    "Determine the hour of the day with the highest number of trips. Display the hour (0-23) and the corresponding trip count.\n",
    "\n",
    "Returns a PySpark DataFrame with a single row showing the hour with the highest trip count and the corresponding number of trips. Use column names: `hour`, `trip_count`.\n",
    "\n",
    "Example output formatting:\n",
    "\n",
    "```\n",
    "+----+----------+\n",
    "|hour|trip_count|\n",
    "+----+----------+\n",
    "|  xx|    xxxxxx|\n",
    "+----+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def busiest_hour(trips):\n",
    "    return (\n",
    "        trips\n",
    "        .withColumn(\"pickup_ts\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "        .withColumn(\"hour\", hour(col(\"pickup_ts\")))\n",
    "        .groupBy(\"hour\")\n",
    "        .agg(count(\"*\").alias(\"trip_count\"))\n",
    "        .orderBy(col(\"trip_count\").desc())\n",
    "        .limit(1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 [5 pts] Update the `most_freq_pickup_locations()` function\n",
    "Top 10 Most Frequent Pickup Locations\n",
    "\n",
    "Identify the top 10 pickup locations (by `PULocationID`) with the highest number of trips. Display the location IDs along with their corresponding trip counts.\n",
    "\n",
    "Return a PySpark DataFrame with the top 10 rows ordered by `trip_count` in descending order. Use column names: `PULocationID`, `trip_count`.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "A table with 10 rows listing the `PULocationID` values and the number of trips observed for each, ordered from most to least frequent.\n",
    "\n",
    "Example output formatting:\n",
    "```\n",
    "+------------+----------+\n",
    "|PULocationID|trip_count|\n",
    "+------------+----------+\n",
    "|         xxx|    xxxxxx|\n",
    "|         xxx|    xxxxxx|\n",
    "|         xxx|    xxxxxx|\n",
    "|         xxx|    xxxxxx|\n",
    "|         ...|    ......|\n",
    "+------------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def most_freq_pickup_locations(trips): \n",
    "    return (\n",
    "        trips\n",
    "        .groupBy(\"PULocationID\")\n",
    "        .agg(count(\"*\").alias(\"trip_count\"))\n",
    "        .orderBy(col(\"trip_count\").desc())\n",
    "        .limit(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 [6 pts] Update the `avg_trip_distance_and_duration()` function\n",
    "Average Trip Distance and Duration by Hour\n",
    "\n",
    "Calculate the average trip distance and average trip duration by pickup hour (0-23), using `tpep_pickup_datetime` for the hour. Display the hour along with the corresponding averages.\n",
    "\n",
    "Compute trip duration in minutes (difference between drop-off time and pickup time).\n",
    "\n",
    "Exclude rows where `tpep_pickup_datetime` or `tpep_dropoff_datetime` is null and where `trip_distance` <= 0. No additional outlier filtering or rounding is required.\n",
    "\n",
    "Note: You can use `unix_timestamp` to help with calculating the duration.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "A table with 24 rows (hours 0-23) showing each hour ordered ascending along with the average trip distance and average trip duration for that hour. Use column names: `hour`, `avg_trip_distance`, `avg_trip_duration`.\n",
    "\n",
    "Example output formatting:\n",
    "```\n",
    "+----+------------------+------------------+\n",
    "|hour| avg_trip_distance| avg_trip_duration|\n",
    "+----+------------------+------------------+\n",
    "|   0|           xxxxxxx|           xxxxxxx|\n",
    "|   1|           xxxxxxx|           xxxxxxx|\n",
    "|   2|           xxxxxxx|           xxxxxxx|\n",
    "|   3|           xxxxxxx|           xxxxxxx|\n",
    "| ...|               ...|               ...|\n",
    "|  23|           xxxxxxx|           xxxxxxx|\n",
    "+----+------------------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def avg_trip_distance_and_duration(trips):\n",
    "    valid = (\n",
    "        trips\n",
    "        .filter(col(\"tpep_pickup_datetime\").isNotNull() & col(\"tpep_dropoff_datetime\").isNotNull() & (col(\"trip_distance\") > 0))\n",
    "        .withColumn(\"pickup_ts\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "        .withColumn(\"dropoff_ts\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "        .withColumn(\"hour\", hour(col(\"pickup_ts\")))\n",
    "        .withColumn(\"duration_minutes\", (unix_timestamp(col(\"dropoff_ts\")) - unix_timestamp(col(\"pickup_ts\"))) / 60.0)\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        valid\n",
    "        .groupBy(\"hour\")\n",
    "        .agg(\n",
    "            avg(col(\"trip_distance\")).alias(\"avg_trip_distance\"),\n",
    "            avg(col(\"duration_minutes\")).alias(\"avg_trip_duration\")\n",
    "        )\n",
    "        .orderBy(col(\"hour\").asc())\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 [10 pts] Update the `most_freq_peak_hour_fares()` function\n",
    "Top 10 Most Frequent Routes During Peak Hours\n",
    "\n",
    "Identify the top 10 most frequent routes (combinations of `PULocationID` and `DOLocationID`) during peak hours and return route-level statistics. For this question:\n",
    "\n",
    "- Peak-hour windows are 7:00 - 8:59 (morning) and 16:00 - 18:59 (evening). Consider trips whose pickup hour falls within these windows.\n",
    "- Exclude rows where `PULocationID` or `DOLocationID` is null.\n",
    "- Exclude routes where `PULocationID` equals `DOLocationID` (a valid route must have different pickup and drop-off locations).\n",
    "\n",
    "For each route, compute:\n",
    "\n",
    "- `trip_count`: number of trips for that route during peak hours, and\n",
    "- `avg_total_fare`: the average of the trip total_amount for that route, rounded to two decimal places.\n",
    "\n",
    "Join the route results with the provided zones dataset to include human-readable zone names for pickup and dropoff. Return a PySpark DataFrame with the following columns (in this order):\n",
    "\n",
    "`PULocationID`, `PUZone`, `DOLocationID`, `DOZone`, `trip_count`, `avg_total_fare`.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "A table with 10 rows showing the top 10 routes during peak hours ordered by `trip_count` descending. If multiple routes share the same `trip_count`, any stable ordering among them is acceptable as long as results are ordered by `trip_count` descending.\n",
    "\n",
    "Example output formatting:\n",
    "```\n",
    "+------------+------+------------+------+----------+--------------+\n",
    "|PULocationID|PUZone|DOLocationID|DOZone|trip_count|avg_total_fare|\n",
    "+------------+------+------------+------+----------+--------------+\n",
    "|xxx         |xxx   |xxx         |xxx   |xxx       |xx.xx         |\n",
    "|xxx         |xxx   |xxx         |xxx   |xxx       |xx.xx         |\n",
    "|xxx         |xxx   |xxx         |xxx   |xxx       |xx.xx         |\n",
    "|...         |...   |...         |...   |...       |...           |\n",
    "+------------+------+------------+------+----------+--------------|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def most_freq_peak_hour_fares(trips, zones):\n",
    "    # Peak hours: 7-8 and 16-18 by pickup hour\n",
    "    peak_trips = (\n",
    "        trips\n",
    "        .filter(col(\"PULocationID\").isNotNull() & col(\"DOLocationID\").isNotNull())\n",
    "        .withColumn(\"pickup_ts\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "        .withColumn(\"pickup_hour\", hour(col(\"pickup_ts\")))\n",
    "        .filter((col(\"pickup_hour\").between(7, 8)) | (col(\"pickup_hour\").between(16, 18)))\n",
    "        .filter(col(\"PULocationID\") != col(\"DOLocationID\"))\n",
    "    )\n",
    "\n",
    "    routes = (\n",
    "        peak_trips\n",
    "        .groupBy(\"PULocationID\", \"DOLocationID\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"trip_count\"),\n",
    "            round(avg(col(\"total_amount\")), 2).alias(\"avg_total_fare\")\n",
    "        )\n",
    "        .orderBy(col(\"trip_count\").desc())\n",
    "        .limit(10)\n",
    "    )\n",
    "\n",
    "    pu = zones.select(col(\"LocationID\").alias(\"PU_LocationID\"), col(\"Zone\").alias(\"PUZone\"))\n",
    "    do = zones.select(col(\"LocationID\").alias(\"DO_LocationID\"), col(\"Zone\").alias(\"DOZone\"))\n",
    "\n",
    "    result = (\n",
    "        routes\n",
    "        .join(pu, routes.PULocationID == pu.PU_LocationID, \"left\")\n",
    "        .join(do, routes.DOLocationID == do.DO_LocationID, \"left\")\n",
    "        .select(\n",
    "            routes.PULocationID,\n",
    "            col(\"PUZone\"),\n",
    "            routes.DOLocationID,\n",
    "            col(\"DOZone\"),\n",
    "            col(\"trip_count\"),\n",
    "            col(\"avg_total_fare\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 [5 pts] q3_output_large.csv\n",
    "The CSV output from running `most_freq_peak_hour_fares(trips, zones)` on the large dataset.\n",
    "\n",
    "- Run the `main` function with `size='large'` to generate the file in the S3 bucket you created in the AWS setup.\n",
    "- Download the generated file from S3 and rename it to `q3_output_large.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission</h2>\n",
    "\n",
    "<p>Once you have finished coding, you can export the notebook from <code>Notebook Explorer</code> by selecting your notebook and clicking <code>Export File</code> from the Actions dropdown.</p>\n",
    "\n",
    "<p>Submit this notebook (q3.ipynb) along with your CSV file (q3_output_large.csv) to Gradescope.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <h2>Testing</h2>\n",
    "\n",
    "  <p>You may use the cell below for any additional testing; however, any code written here will not be run or used during grading.</p>\n",
    "\n",
    "  <ul>\n",
    "    <li>You can run the <code>main</code> function on different dataset sizes to test your work, or run the functions individually as shown in the examples.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trips, zones = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msmall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(size)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load Trip Data\u001b[39;00m\n\u001b[32m     14\u001b[39m trips_path = \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m+size+\u001b[33m'\u001b[39m\u001b[33m/yellow_tripdata*\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m trips = \u001b[43mspark\u001b[49m.read.csv(input_bucket + trips_path, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load Zone Data\u001b[39;00m\n\u001b[32m     18\u001b[39m zones_path = \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m+size+\u001b[33m'\u001b[39m\u001b[33m/taxi*\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "trips, zones = load_data('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trips' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ts = trip_statistics(\u001b[43mtrips\u001b[49m)\n\u001b[32m      2\u001b[39m ts.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'trips' is not defined"
     ]
    }
   ],
   "source": [
    "ts = trip_statistics(trips)\n",
    "ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlarge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms3://cse6242-gburdell3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(size, bucket)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m(size, bucket):\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Runs your functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     trips, zones = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUser:\u001b[39m\u001b[33m\"\u001b[39m, user())\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(size)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load Trip Data\u001b[39;00m\n\u001b[32m     14\u001b[39m trips_path = \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m+size+\u001b[33m'\u001b[39m\u001b[33m/yellow_tripdata*\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m trips = \u001b[43mspark\u001b[49m.read.csv(input_bucket + trips_path, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load Zone Data\u001b[39;00m\n\u001b[32m     18\u001b[39m zones_path = \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m+size+\u001b[33m'\u001b[39m\u001b[33m/taxi*\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "main('large', 's3://cse6242-gburdell3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
