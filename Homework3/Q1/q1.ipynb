{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5905a69",
   "metadata": {},
   "source": [
    "# CSE6242 - HW3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5717e-fb7f-415c-ae02-16459c544fa4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remove any comment that says \"#export\" because that will crash the autograder in Gradescope. We use this comment to export your code in these cells for grading.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09289981",
   "metadata": {},
   "source": [
    "Pyspark Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "### DO NOT MODIFY THIS CELL ###\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import hour, when, col, date_format, to_timestamp, ceil, coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9e0f8",
   "metadata": {},
   "source": [
    "Initialize PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c18c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=HW3-Q1, master=local[*]) created by __init__ at /tmp/ipykernel_24/1694043449.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### DO NOT MODIFY THIS CELL ###\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHW3-Q1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m SQLContext(sc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=HW3-Q1, master=local[*]) created by __init__ at /tmp/ipykernel_24/1694043449.py:2 "
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "sc = pyspark.SparkContext(appName=\"HW3-Q1\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ae314",
   "metadata": {},
   "source": [
    "Define function for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e5bbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "def load_data():\n",
    "    df = sqlContext.read.option(\"header\",True) \\\n",
    "     .csv(\"yellow_tripdata_2019-01_short.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52409d",
   "metadata": {},
   "source": [
    "### Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f6e00",
   "metadata": {},
   "source": [
    "Perform data casting to clean incoming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with the all the original columns\n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"int\"))\n",
    "        .withColumn(\"total_amount\", col(\"total_amount\").cast(\"float\"))\n",
    "        .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"float\"))\n",
    "        .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"float\"))\n",
    "        .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"float\"))\n",
    "        .withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "        .withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "    )\n",
    "\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f565d0",
   "metadata": {},
   "source": [
    "### Q1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4f712",
   "metadata": {},
   "source": [
    "Find rate per person for based on how many passengers travel between pickup and dropoff locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e115152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def common_pair(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - PULocationID\n",
    "            - DOLocationID\n",
    "            - total_passenger_count\n",
    "            - per_person_rate\n",
    "            \n",
    "    per_person_rate is the total_amount per person for a given pair.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    df_filtered = df.where(col(\"PULocationID\") != col(\"DOLocationID\"))\n",
    "\n",
    "    agg_df = (\n",
    "        df_filtered\n",
    "        .groupBy(\"PULocationID\", \"DOLocationID\")\n",
    "        .agg(\n",
    "            F.sum(\"passenger_count\").alias(\"total_passenger_count\"),\n",
    "            F.sum(\"total_amount\").alias(\"sum_total_amount\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"per_person_rate\",\n",
    "            F.when(col(\"total_passenger_count\") > 0,\n",
    "                   col(\"sum_total_amount\") / col(\"total_passenger_count\"))\n",
    "             .otherwise(F.lit(None))\n",
    "        )\n",
    "        .select(\n",
    "            \"PULocationID\",\n",
    "            \"DOLocationID\",\n",
    "            \"total_passenger_count\",\n",
    "            \"per_person_rate\",\n",
    "        )\n",
    "        .orderBy(col(\"total_passenger_count\").desc(), col(\"per_person_rate\").desc())\n",
    "        .limit(10)\n",
    "    )\n",
    "\n",
    "    df = agg_df\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127574ab",
   "metadata": {},
   "source": [
    "### Q1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8fd27",
   "metadata": {},
   "source": [
    "Find trips which trip distances generate the highest tip percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "376c981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def distance_with_most_tip(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - trip_distance\n",
    "            - tip_percent\n",
    "            \n",
    "    trip_percent is the percent of tip out of fare_amount\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    filtered = df.where((col(\"fare_amount\") > 2.0) & (col(\"trip_distance\") > 0))\n",
    "\n",
    "    with_tip = filtered.withColumn(\n",
    "        \"tip_percent\",\n",
    "        (col(\"tip_amount\") * F.lit(100.0)) / col(\"fare_amount\")\n",
    "    )\n",
    "\n",
    "    by_distance = (\n",
    "        with_tip\n",
    "        .withColumn(\"rounded_distance\", F.ceil(col(\"trip_distance\")))\n",
    "        .groupBy(\"rounded_distance\")\n",
    "        .agg(F.avg(\"tip_percent\").alias(\"tip_percent\"))\n",
    "        .orderBy(col(\"tip_percent\").desc())\n",
    "        .limit(15)\n",
    "        .select(col(\"rounded_distance\").alias(\"trip_distance\"), col(\"tip_percent\"))\n",
    "    )\n",
    "\n",
    "    df = by_distance\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0172fe6",
   "metadata": {},
   "source": [
    "### Q1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613c906",
   "metadata": {},
   "source": [
    "Determine the average speed at different times of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abff9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def time_with_most_traffic(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - time_of_day\n",
    "            - am_avg_speed\n",
    "            - pm_avg_speed\n",
    "            \n",
    "    am_avg_speed and pm_avg_speed are the average trip distance / average trip time calculated for each hour\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    with_time = (\n",
    "        df\n",
    "        .withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "        .withColumn(\"time_of_day\", date_format(col(\"tpep_pickup_datetime\"), \"K\"))\n",
    "        .withColumn(\n",
    "            \"period\",\n",
    "            when(col(\"pickup_hour\") < 12, F.lit(\"AM\")).otherwise(F.lit(\"PM\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"duration_hours\",\n",
    "            (col(\"tpep_dropoff_datetime\").cast(\"long\") - col(\"tpep_pickup_datetime\").cast(\"long\")) / F.lit(3600.0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    per_group = (\n",
    "        with_time\n",
    "        .groupBy(\"time_of_day\", \"period\")\n",
    "        .agg(\n",
    "            F.avg(\"trip_distance\").alias(\"avg_trip_distance\"),\n",
    "            F.avg(\"duration_hours\").alias(\"avg_duration_hours\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"avg_speed\",\n",
    "            when(col(\"avg_duration_hours\") > 0,\n",
    "                 col(\"avg_trip_distance\") / col(\"avg_duration_hours\")).otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .select(\"time_of_day\", \"period\", \"avg_speed\")\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        per_group\n",
    "        .groupBy(\"time_of_day\")\n",
    "        .pivot(\"period\", [\"AM\", \"PM\"]).agg(F.first(\"avg_speed\"))\n",
    "        .withColumnRenamed(\"AM\", \"am_avg_speed\")\n",
    "        .withColumnRenamed(\"PM\", \"pm_avg_speed\")\n",
    "    )\n",
    "\n",
    "    # order rows by time_of_day numerically 0..11\n",
    "    result = result.withColumn(\"_td\", col(\"time_of_day\").cast(\"int\")).orderBy(col(\"_td\")).drop(\"_td\")\n",
    "\n",
    "    df = result\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b4e83-0f93-4637-bc3b-34f9fbb9f249",
   "metadata": {},
   "source": [
    "## The below cells are for you to investigate your solutions and will not be graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3b238c9-7bc7-458a-a3d8-8ce2d686418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbab81e-2317-4b4e-b25a-88f3110a94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------------+------------------+\n",
      "|PULocationID|DOLocationID|total_passenger_count|   per_person_rate|\n",
      "+------------+------------+---------------------+------------------+\n",
      "|         239|         238|                   62|  4.26274198870505|\n",
      "|         237|         236|                   60| 4.482500068346659|\n",
      "|         263|         141|                   52|3.4190384974846473|\n",
      "|         161|         236|                   42| 5.368571440378825|\n",
      "|         148|          79|                   42| 4.711904752822149|\n",
      "|         142|         238|                   39|  5.05487182812813|\n",
      "|         141|         236|                   37| 4.355675723101641|\n",
      "|         239|         143|                   37| 4.252162224537617|\n",
      "|         239|         142|                   35| 3.817714350564139|\n",
      "|          79|         170|                   34| 6.394705884596881|\n",
      "+------------+------------+---------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "common_pair(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf7dd12b-4b60-407b-9c52-5b7cb2082cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|trip_distance|       tip_percent|\n",
      "+-------------+------------------+\n",
      "|            1|17.129815992473326|\n",
      "|            2| 15.81552712528758|\n",
      "|           17|15.796441904884075|\n",
      "|           20|15.112410000027054|\n",
      "|            3|14.886705735873237|\n",
      "|            6|14.579695033034238|\n",
      "|            5|14.245405810737791|\n",
      "|            4|13.831569499212133|\n",
      "|            9|13.814476541860179|\n",
      "|            8| 12.07259673796427|\n",
      "|           19| 11.95263232603509|\n",
      "|           10|11.880490472296412|\n",
      "|            7|10.800575637356776|\n",
      "|           21| 10.73901997840823|\n",
      "|           18|10.696822232896201|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_with_most_tip(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a02723df-2490-4234-9292-eea7cebb08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------------+\n",
      "|time_of_day|      am_avg_speed|       pm_avg_speed|\n",
      "+-----------+------------------+-------------------+\n",
      "|          0| 9.377696196631234|               NULL|\n",
      "|          1|10.845483413697353|  5.125214305177561|\n",
      "|          3|              NULL|                0.0|\n",
      "|          4|              NULL|                0.0|\n",
      "|          5|              NULL| 0.5137660239764732|\n",
      "|          6|              NULL|  9.989847870647605|\n",
      "|          7|              NULL|0.18415305490417713|\n",
      "|          8|              NULL| 0.5183127622697896|\n",
      "|         10|              NULL| 0.6147483972627696|\n",
      "|         11|              NULL|  4.650958285207579|\n",
      "+-----------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_with_most_traffic(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf32b0e-c739-4ede-ba49-84c3c78ed172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
